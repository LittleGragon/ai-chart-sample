# WebLLM AI Chat Sample

一个基于 WebLLM 的高性能浏览器内 AI 聊天应用，支持多种开源大语言模型。

## 🌟 功能特点

- **纯浏览器运行**：无需服务器，完全在浏览器内运行
- **WebGPU 加速**：利用 GPU 硬件加速，提供快速推理
- **隐私保护**：所有数据本地处理，不上传任何信息
- **多模型支持**：支持 Llama、DeepSeek、Phi、Hermes 等多种模型
- **中文界面**：完整的中文界面和提示
- **实时对话**：支持流式输出，实时对话体验

## 🚀 快速开始

### 1. 启动应用

```bash
# 使用 Python 启动本地服务器
python3 -m http.server 8000

# 或使用 Node.js
npx serve .
```

### 2. 访问应用

打开浏览器访问：http://localhost:8000

### 3. 使用步骤

1. **选择模型**：从下拉菜单选择合适的模型
2. **加载模型**：点击"加载模型"按钮
3. **等待初始化**：首次加载需要下载模型权重
4. **开始对话**：输入消息并开始与 AI 对话

## 📊 支持的模型

### Llama 系列
- **Llama-3.2-1B**：轻量级，适合低配置设备
- **Llama-3.2-3B**：平衡性能与资源占用
- **Llama-3.1-8B**：高性能，推荐日常使用

### DeepSeek-R1 系列
- **DeepSeek-R1-Distill-Qwen-7B**：专注推理能力
- **DeepSeek-R1-Distill-Llama-8B**：Llama 版本推理模型

### Phi 系列
- **Phi-3.5-mini**：微软轻量级模型
- **Phi-3.5-vision**：支持图像理解的多模态模型

### Hermes 系列
- **Hermes-3-Llama**：通用助手模型
- **Hermes-2-Pro**：专业版本

## 💻 系统要求

### 浏览器支持
- **Chrome 113+** 或 **Edge 113+**
- **WebGPU 支持**（必需）
- **推荐配置**：8GB+ 内存，支持 WebGPU 的显卡

### 最低配置
- **内存**：4GB（使用 1B 模型）
- **显卡**：支持 WebGPU 的集成显卡
- **浏览器**：最新版 Chrome/Edge

## 🎯 使用建议

### 新手推荐
- **Llama-3.2-1B-Instruct-q4f16_1-MLC**：879MB VRAM，快速启动

### 日常使用
- **Llama-3.1-8B-Instruct-q4f16_1-MLC**：5GB VRAM，性能优秀

### 专业场景
- **DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC**：推理能力强
- **Hermes-3-Llama-3.1-8B-q4f16_1-MLC**：通用助手

## 📈 性能对比

| 模型 | VRAM 占用 | 响应速度 | 推理能力 | 推荐场景 |
|---|---|---|---|---|
| Llama-3.2-1B | 879MB | 快 | 基础 | 测试/低配置 |
| Llama-3.2-3B | 2.3GB | 中等 | 良好 | 日常使用 |
| Llama-3.1-8B | 5GB | 中等 | 优秀 | 生产环境 |
| DeepSeek-R1 | 5GB | 中等 | 极强 | 复杂推理 |

## 🛠️ 常见问题

### Q: 为什么模型加载失败？
A: 可能原因：
- 浏览器不支持 WebGPU（使用 Chrome/Edge 最新版）
- 网络连接问题（检查网络并重试）
- 内存不足（选择更小的模型）

### Q: 首次加载为什么这么慢？
A: 首次使用需要下载模型权重文件（几百 MB 到几 GB），后续会缓存到本地。

### Q: 如何切换模型？
A: 重新选择模型并点击"加载模型"按钮即可。

### Q: 支持中文吗？
A: 支持！所有模型都支持中英文对话。

## 🔄 更新日志

### v1.1.0 (当前)
- ✅ 修复模型标识符错误
- ✅ 更新为 WebLLM v0.2.79 支持的模型
- ✅ 添加模型分组和 VRAM 信息显示
- ✅ 改进错误处理和用户提示
- ✅ 添加 WebGPU 兼容性检查

### v1.0.0
- ✅ 初始版本发布
- ✅ 基础聊天功能
- ✅ 模型选择界面