<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chat Sample with WebLLM</title>
  <link rel="icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR4nGP4////BwAC/gL9V4q7YQAAAABJRU5ErkJggg==" />
  <!-- Material Fonts and Icons -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,400,0,0" rel="stylesheet">
  <style>
    :root { color-scheme: light dark; }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: Roboto, system-ui, -apple-system, "Segoe UI", Arial, "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
      background: Canvas;
      color: CanvasText;
    }
    main.container {
      max-width: 880px;
      margin: 0 auto;
      padding: 20px 24px 28px;
      display: flex;
      flex-direction: column;
      gap: 20px;
    }
    header.app-header {
      display: flex;
      align-items: center;
      gap: 12px;
      padding: 8px 0 4px;
    }
    header.app-header h1 {
      font-size: 20px;
      font-weight: 600;
      margin: 0;
    }
    .material-symbols-rounded { font-variation-settings: 'FILL' 0, 'wght' 400, 'GRAD' 0, 'opsz' 24; }
    #model-selection-container {
      display: flex;
      align-items: center;
      gap: 12px;
      flex-wrap: wrap;
      padding: 8px 0;
    }
    #model-selection {
      min-width: 260px;
      padding: 12px 14px;
      padding-right: 40px;
      border-radius: 12px;
      border: 1px solid #d6d6d6;
      background: inherit;
      color: inherit;
      appearance: none;
      -webkit-appearance: none;
      -moz-appearance: none;
      background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="%23888" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg>');
      background-repeat: no-repeat;
      background-position: right 12px center;
      background-size: 16px;
      font-size: 14px;
      line-height: 1.4;
      cursor: pointer;
      transition: border-color .15s ease, box-shadow .15s ease, background-color .15s ease;
    }
    #model-selection:hover { border-color: #bdbdbd; }
    #model-selection:focus { outline: none; border-color: #9fa8da; box-shadow: 0 0 0 3px color-mix(in oklab, CanvasText 15%, transparent); }
    #model-selection:disabled { cursor: not-allowed; opacity: .7; }
    #model-selection optgroup { font-weight: 600; color: inherit; }
    #model-selection option { color: CanvasText; }
    #loading-row {
      display: flex;
      align-items: center;
      gap: 10px;
      min-height: 28px;
    }
    #loading-message {
      font-size: 14px;
      opacity: 0.8;
      display: none;
    }
    .elevated-card {
      padding: 16px 18px;
      border-radius: 12px;
      border: 1px solid color-mix(in oklab, CanvasText 10%, transparent);
      background: color-mix(in oklab, Canvas 96%, CanvasText 4%);
    }
    #chat-container {
      height: 420px;
      overflow: auto;
      display: flex;
      flex-direction: column;
      gap: 8px;
    }
    .message {
      padding: 12px 14px;
      border-radius: 12px;
      line-height: 1.5;
      word-wrap: break-word;
      white-space: pre-wrap;
      max-width: 80%;
    }
    .user-message {
      align-self: flex-end;
      background: rgba(25, 118, 210, 0.12);
    }
    .ai-message {
      align-self: flex-start;
      background: rgba(0, 0, 0, 0.06);
    }
    #input-container {
      display: flex;
      gap: 12px;
      align-items: flex-end;
    }
    #user-input {
      flex: 1;
      min-height: 44px;
      padding: 12px 14px;
      border-radius: 12px;
      border: 1px solid #d6d6d6;
      background: inherit;
      color: inherit;
      resize: vertical;
    }
    .spinner {
      width: 24px;
      height: 24px;
      border: 3px solid color-mix(in oklab, CanvasText 30%, transparent);
      border-top-color: color-mix(in oklab, CanvasText 80%, transparent);
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }
    @keyframes spin {
      to { transform: rotate(360deg); }
    }
    button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      border-radius: 12px;
      border: 1px solid color-mix(in oklab, CanvasText 20%, transparent);
      background: color-mix(in oklab, Canvas 94%, CanvasText 6%);
      color: inherit;
      cursor: pointer;
      transition: background-color .2s ease, border-color .2s ease, box-shadow .2s ease, transform .05s ease;
    }
    button:hover { border-color: color-mix(in oklab, CanvasText 35%, transparent); }
    button:active { transform: translateY(1px); }
    button:disabled { opacity: 0.6; cursor: not-allowed; }

    #load-model-button {
      border: none;
      background: #1a73e8;
      color: #fff;
      box-shadow: 0 1px 2px rgba(0,0,0,.2), 0 2px 6px rgba(26,115,232,.3);
      font-weight: 600;
    }
    #load-model-button:hover {
      background: #1669c1;
      box-shadow: 0 2px 4px rgba(0,0,0,.25), 0 4px 10px rgba(26,115,232,.35);
    }
    #load-model-button:active { background: #155ab0; }
    #load-model-button:disabled {
      background: color-mix(in oklab, #1a73e8 40%, Canvas 60%);
      color: color-mix(in oklab, #fff 70%, CanvasText 30%);
      box-shadow: none;
    }
    @media (min-width: 768px) {
      main.container { padding: 28px 32px 36px; gap: 24px; }
      .elevated-card { padding: 20px 22px; }
      .message { padding: 14px 16px; }
      #user-input { min-height: 52px; padding: 14px 16px; }
      button { padding: 12px 18px; }
      #model-selection { padding: 12px 16px; }
    }
    /* Markdown styling */
    .message h1,.message h2,.message h3{margin:0.6em 0 0.4em;}
    .message h4,.message h5,.message h6{margin:0.5em 0 0.3em;}
    .message h1{font-size:1.3rem;}
    .message h2{font-size:1.2rem;}
    .message h3{font-size:1.1rem;}
    .message pre{
      background: color-mix(in oklab, CanvasText 10%, transparent);
      padding: 12px 14px;
      border-radius: 10px;
      overflow: auto;
      line-height: 1.4;
    }
    .message code{
      background: color-mix(in oklab, CanvasText 10%, transparent);
      padding: 2px 6px;
      border-radius: 8px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace;
    }
    .message pre code{ background: transparent; padding: 0; }
    .message a{ color: #1a73e8; text-decoration: underline; }
    .message ul,.message ol{ padding-left: 1.25em; margin: 0.2em 0 0.6em; }
    .message li{ margin: 0.15em 0; }
    .message blockquote{
      border-left: 3px solid color-mix(in oklab, CanvasText 25%, transparent);
      padding: 6px 10px;
      margin: 0.4em 0;
      color: color-mix(in oklab, CanvasText 80%, transparent);
    }
  </style>
</head>
<body>
  <main class="container">
    <header class="app-header">
      <span class="material-symbols-rounded">smart_toy</span>
      <h1>WebLLM Chat</h1>
    </header>

    <section id="model-selection-container">
      <label for="model-selection">é€‰æ‹©æ¨¡å‹:</label>
      <select id="model-selection">
        <optgroup label="Llama-3.2 (1B) - è½»é‡çº§">
          <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama-3.2-1B q4f16_1 (879MB VRAM)</option>
          <option value="Llama-3.2-1B-Instruct-q4f32_1-MLC">Llama-3.2-1B q4f32_1 (1.1GB VRAM)</option>
        </optgroup>
        <optgroup label="Llama-3.2 (3B) - å¹³è¡¡å‹">
          <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama-3.2-3B q4f16_1 (2.3GB VRAM)</option>
          <option value="Llama-3.2-3B-Instruct-q4f32_1-MLC">Llama-3.2-3B q4f32_1 (3GB VRAM)</option>
        </optgroup>
        <optgroup label="Llama-3.1 (8B) - é«˜æ€§èƒ½">
          <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama-3.1-8B q4f16_1 (5GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama-3.1-8B q4f32_1 (6.1GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC-1k">Llama-3.1-8B q4f16_1-1k (4.6GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC-1k">Llama-3.1-8B q4f32_1-1k (5.3GB VRAM)</option>
        </optgroup>
        <optgroup label="DeepSeek-R1 - æ¨ç†ä¸“å®¶">
          <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek-R1-Qwen-7B q4f16_1 (5.1GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Qwen-7B-q4f32_1-MLC">DeepSeek-R1-Qwen-7B q4f32_1 (5.9GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek-R1-Llama-8B q4f16_1 (5GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Llama-8B-q4f32_1-MLC">DeepSeek-R1-Llama-8B q4f32_1 (6.1GB VRAM)</option>
        </optgroup>
        <optgroup label="Hermes-3 - é€šç”¨åŠ©æ‰‹">
          <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes-3-Llama-3.2-3B q4f16_1 (2.3GB VRAM)</option>
          <option value="Hermes-3-Llama-3.2-3B-q4f32_1-MLC">Hermes-3-Llama-3.2-3B q4f32_1 (3GB VRAM)</option>
          <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes-3-Llama-3.1-8B q4f16_1 (4.9GB VRAM)</option>
          <option value="Hermes-3-Llama-3.1-8B-q4f32_1-MLC">Hermes-3-Llama-3.1-8B q4f32_1 (5.8GB VRAM)</option>
        </optgroup>
        <optgroup label="Phi-3.5 - å¾®è½¯æ¨¡å‹">
          <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5-mini q4f16_1 (3.7GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f32_1-MLC">Phi-3.5-mini q4f32_1 (5.5GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f16_1-MLC-1k">Phi-3.5-mini q4f16_1-1k (2.5GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f32_1-MLC-1k">Phi-3.5-mini q4f32_1-1k (3.2GB VRAM)</option>
        </optgroup>
        <optgroup label="Mistral - æ¬§æ´²æ¨¡å‹">
          <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes-2-Pro-Mistral-7B q4f16_1 (4GB VRAM)</option>
        </optgroup>
        <optgroup label="è§†è§‰æ¨¡å‹ (VLM)">
          <option value="Phi-3.5-vision-instruct-q4f16_1-MLC">Phi-3.5-vision q4f16_1 (4GB VRAM)</option>
          <option value="Phi-3.5-vision-instruct-q4f32_1-MLC">Phi-3.5-vision q4f32_1 (5.9GB VRAM)</option>
        </optgroup>
      </select>
      <button id="load-model-button" disabled>
        <span class="material-symbols-rounded">download</span>
        åŠ è½½æ¨¡å‹
      </button>
    </section>

    <div id="loading-row">
      <div id="model-progress" class="spinner" style="display: none;"></div>
      <div id="loading-message" class="loading"></div>
    </div>

    <div class="elevated-card">
      <div id="chat-container"></div>
    </div>

    <div id="input-container">
      <textarea id="user-input" placeholder="è¾“å…¥æ¶ˆæ¯â€¦ï¼ˆæŒ‰ Enter å‘é€ï¼‰"></textarea>
      <button id="send-button" disabled>
        <span class="material-symbols-rounded">send</span>
        å‘é€
      </button>
    </div>
    <script type="module">
let webllm = null;

const chatContainer = document.getElementById('chat-container');
const userInput = document.getElementById('user-input');
const sendButton = document.getElementById('send-button');
const modelSelection = document.getElementById('model-selection');
const loadModelButton = document.getElementById('load-model-button');
const loadingMessage = document.getElementById('loading-message');
const modelProgress = document.getElementById('model-progress');

let messages = [
  { role: "system", content: "You are a helpful AI assistant." }
];

let engine = null;

function initProgressCallback(report) {
  const progress = report.progress ? ` (${(report.progress * 100).toFixed(1)}%)` : '';
  loadingMessage.textContent = `æ­£åœ¨åŠ è½½æ¨¡å‹: ${report.text}${progress}`;
  loadingMessage.style.display = 'block';
  modelProgress.style.display = 'inline-block';
  chatContainer.scrollTop = chatContainer.scrollHeight;
}

async function loadModel() {
  // Lazy import WebLLM when needed
  if (!webllm) {
    try {
      webllm = await import("https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.79/+esm");
    } catch (e) {
      loadingMessage.textContent = "æ— æ³•åŠ è½½ WebLLMï¼ˆå¯èƒ½è¢«é¢„è§ˆç¯å¢ƒæˆ–ç½‘ç»œé™åˆ¶æ‹¦æˆªï¼‰ã€‚è¯·åœ¨å¤–éƒ¨æµè§ˆå™¨æ‰“å¼€ http://127.0.0.1:8001 å†è¯•ã€‚";
      loadingMessage.style.display = "block";
      return;
    }
  }
  // Clear chat container
  chatContainer.innerHTML = '';
  
  // Reset messages
  messages = [
    { role: "system", content: "You are a helpful AI assistant." }
  ];
  
  // Get selected model
  const selectedModel = modelSelection.value;
  
  // Disable UI elements during loading
  modelSelection.disabled = true;
  loadModelButton.disabled = true;
  sendButton.disabled = true;
  modelProgress.style.display = 'inline-block';
  loadingMessage.style.display = 'block';
  loadingMessage.textContent = 'æ­£åœ¨å‡†å¤‡åŠ è½½æ¨¡å‹â€¦';
  
  try {
    // Create or reload engine
    if (engine === null) {
      engine = await webllm.CreateMLCEngine(
        selectedModel,
        { initProgressCallback: initProgressCallback }
      );
    } else {
      await engine.reload(selectedModel, { initProgressCallback: initProgressCallback });
    }
    
    // Hide loading indicators
    loadingMessage.style.display = 'none';
    modelProgress.style.display = 'none';
    
    // Enable send button
    sendButton.disabled = false;
    
    // Add welcome message with model info
    addMessage('ai', `âœ… æ¨¡å‹ ${selectedModel} å·²åŠ è½½æˆåŠŸï¼\n\nğŸ’¡ æç¤ºï¼š\n- å½“å‰æ¨¡å‹æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡å¯¹è¯\n- ç‚¹å‡»å‘é€æŒ‰é’®æˆ–æŒ‰ Enter é”®å‘é€æ¶ˆæ¯\n- å¦‚éœ€åˆ‡æ¢æ¨¡å‹ï¼Œè¯·é‡æ–°é€‰æ‹©å¹¶ç‚¹å‡»"åŠ è½½æ¨¡å‹"\n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ`);
  } catch (error) {
    console.error('Failed to load model:', error);
    
    let errorMessage = 'æ¨¡å‹åŠ è½½å¤±è´¥';
    if (error.message && error.message.includes('WebGPU')) {
      errorMessage = 'âŒ æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒ WebGPUï¼Œè¯·ä½¿ç”¨ Chrome/Edge æœ€æ–°ç‰ˆæœ¬';
    } else if (error.message && error.message.includes('Network')) {
      errorMessage = 'âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•';
    } else if (error.message && error.message.includes('memory')) {
      errorMessage = 'âŒ å†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•é€‰æ‹©æ›´å°çš„æ¨¡å‹';
    } else if (error.message) {
      errorMessage = `âŒ åŠ è½½å¤±è´¥ï¼š${error.message}`;
    }
    
    loadingMessage.textContent = errorMessage;
    loadingMessage.style.display = 'block';
    modelProgress.style.display = 'none';
    
    // Enable model selection
    modelSelection.disabled = false;
    loadModelButton.disabled = false;
  }
}

// Enable the load model button when the page loads
window.addEventListener('load', async () => {
  loadModelButton.disabled = false;
  
  // Check WebGPU support
  if (!navigator.gpu) {
    addMessage('ai', 'âš ï¸ è­¦å‘Šï¼šæ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒ WebGPU\n\nWebLLM éœ€è¦ WebGPU æ”¯æŒæ‰èƒ½è¿è¡Œã€‚è¯·ä½¿ç”¨ä»¥ä¸‹æµè§ˆå™¨ï¼š\n- Chrome 113+ æˆ– Edge 113+\n- Chrome Canary (æœ€æ–°æµ‹è¯•ç‰ˆ)\n- å…¶ä»–æ”¯æŒ WebGPU çš„ç°ä»£æµè§ˆå™¨\n\nå½“å‰æµè§ˆå™¨å°†æ— æ³•åŠ è½½æ¨¡å‹ã€‚');
    loadModelButton.disabled = true;
  } else {
    addMessage('ai', 'ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ WebLLM AI èŠå¤©ï¼\n\nğŸ“‹ ä½¿ç”¨æ­¥éª¤ï¼š\n1. ä»ä¸‹æ‹‰èœå•é€‰æ‹©ä¸€ä¸ªæ¨¡å‹\n2. ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®\n3. ç­‰å¾…æ¨¡å‹ä¸‹è½½å’Œåˆå§‹åŒ–\n4. å¼€å§‹å¯¹è¯ï¼\n\nğŸ’¡ å»ºè®®ï¼š\n- æ–°æ‰‹æ¨è Llama-3.2-1B ç³»åˆ—ï¼ˆå†…å­˜å ç”¨å°ï¼‰\n- éœ€è¦æ›´å¥½æ€§èƒ½é€‰æ‹© Llama-3.1-8B ç³»åˆ—\n- é¦–æ¬¡åŠ è½½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…');
  }
});

/* Markdown renderer (escapes HTML, supports code fences, inline code, headings, lists, links, hr) */
function renderMarkdown(md) {
  if (!md) return '';
  let s = String(md)
    .replace(/&/g, '&')
    .replace(/</g, '<')
    .replace(/>/g, '>');

  // Extract fenced code blocks first to avoid further processing inside them
  const blocks = [];
  s = s.replace(/```(\w+)?\n([\s\S]*?)```/g, (m, lang, code) => {
    const idx = blocks.length;
    const ln = (lang || '').toLowerCase();
    blocks.push(`<pre><code class="language-${ln}">${code}</code></pre>`);
    return `\u0000CODEBLOCK_${idx}\u0000`;
  });

  // Inline code
  s = s.replace(/`([^`]+)`/g, '<code>$1</code>');

  // Headings
  s = s.replace(/^(#{1,6})\s*(.+)$/gm, (m, hashes, title) => {
    const level = hashes.length;
    return `<h${level}>${title}</h${level}>`;
  });

  // Blockquotes
  s = s.replace(/^>\s?(.*)$/gm, '<blockquote>$1</blockquote>');

  // Horizontal rule
  s = s.replace(/^\s*(?:-{3,}|\*{3,})\s*$/gm, '<hr/>');

  // Links [text](url) with basic allowlist and escaping
  s = s.replace(/\[([^\]]+)\]\(([^)]+)\)/g, (m, text, url) => {
    try {
      const u = new URL(url, window.location.href);
      const ok = u.protocol === 'http:' || u.protocol === 'https:' || u.protocol === 'mailto:';
      if (!ok) return text;
      const escText = String(text).replace(/"/g, '"');
      const href = u.href.replace(/&/g, '&').replace(/"/g, '"');
      return `<a href="${href}" target="_blank" rel="noopener noreferrer">${escText}</a>`;
    } catch {
      return text;
    }
  });

  // Simple list handling and line breaks, preserving code blocks
  const lines = s.split('\n');
  let out = '';
  let inUl = false, inOl = false;
  const tokenRe = /^\u0000CODEBLOCK_(\d+)\u0000$/;

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];

    if (tokenRe.test(line)) {
      if (inUl) { out += '</ul>'; inUl = false; }
      if (inOl) { out += '</ol>'; inOl = false; }
      out += line;
      continue;
    }

    if (/^\s*[-*+]\s+/.test(line)) {
      if (!inUl) { if (inOl) { out += '</ol>'; inOl = false; } out += '<ul>'; inUl = true; }
      out += `<li>${line.replace(/^\s*[-*+]\s+/, '')}</li>`;
      continue;
    }
    if (/^\s*\d+\.\s+/.test(line)) {
      if (!inOl) { if (inUl) { out += '</ul>'; inUl = false; } out += '<ol>'; inOl = true; }
      out += `<li>${line.replace(/^\s*\d+\.\s+/, '')}</li>`;
      continue;
    }

    if (inUl) { out += '</ul>'; inUl = false; }
    if (inOl) { out += '</ol>'; inOl = false; }

    if (line.trim() === '') {
      out += '<br/>';
    } else {
      out += line + '<br/>';
    }
  }
  if (inUl) out += '</ul>';
  if (inOl) out += '</ol>';

  // Restore code blocks
  out = out.replace(/\u0000CODEBLOCK_(\d+)\u0000/g, (_, i) => blocks[Number(i)] || '');

  return out;
}

// Add message to chat container
function addMessage(role, content) {
  const messageElement = document.createElement('div');
  messageElement.className = `message ${role}-message`;
  if (role === 'ai') {
    messageElement.innerHTML = renderMarkdown(content || '');
  } else {
    messageElement.textContent = content || '';
  }
  chatContainer.appendChild(messageElement);
  chatContainer.scrollTop = chatContainer.scrollHeight;
}

// Send message to AI
async function sendChatMessage() {
  const message = (userInput.value || '').trim();
  if (!message || !engine) return;

  // Add user message to UI
  addMessage('user', message);
  
  // Clear input
  userInput.value = '';
  
  // Add user message to history
  messages.push({ role: 'user', content: message });
  
  // Show loading indicator
  const loadingEl = document.createElement('div');
  loadingEl.className = 'message loading';
  loadingEl.textContent = 'AI æ­£åœ¨æ€è€ƒ...';
  chatContainer.appendChild(loadingEl);
  chatContainer.scrollTop = chatContainer.scrollHeight;
  
  try {
    // Generate AI response
    const reply = await engine.chat.completions.create({
      messages: messages,
    });
    
    // Remove loading message
    chatContainer.removeChild(loadingEl);
    
    // Add AI response to UI
    const aiMessage = reply.choices[0].message.content;
    addMessage('ai', aiMessage);
    
    // Add AI response to history
    messages.push({ role: 'assistant', content: aiMessage });
  } catch (error) {
    console.error('Failed to generate response:', error);
    chatContainer.removeChild(loadingEl);
    addMessage('ai', 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚è¯·ç¨åé‡è¯•ã€‚');
  }
}

// Event listeners
sendButton.addEventListener('click', sendChatMessage);
userInput.addEventListener('keydown', (e) => {
  if (e.key === 'Enter' && !e.shiftKey) {
    e.preventDefault();
    sendChatMessage();
  }
});
loadModelButton.addEventListener('click', loadModel);

// Initialize the engine when the page loads
// window.addEventListener('load', initializeEngine);
    </script>
  </main>
</body>
</html>