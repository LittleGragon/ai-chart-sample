<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chat Sample with WebLLM</title>
  <link rel="icon" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR4nGP4////BwAC/gL9V4q7YQAAAABJRU5ErkJggg==" />
  <!-- Material Fonts and Icons -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,400,0,0" rel="stylesheet">
  <style>
    :root { color-scheme: light dark; }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: Roboto, system-ui, -apple-system, "Segoe UI", Arial, "Noto Sans", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
      background: Canvas;
      color: CanvasText;
    }
    main.container {
      max-width: 880px;
      margin: 0 auto;
      padding: 20px 24px 28px;
      display: flex;
      flex-direction: column;
      gap: 20px;
    }
    header.app-header {
      display: flex;
      align-items: center;
      gap: 12px;
      padding: 8px 0 4px;
    }
    header.app-header h1 {
      font-size: 20px;
      font-weight: 600;
      margin: 0;
    }
    .material-symbols-rounded { font-variation-settings: 'FILL' 0, 'wght' 400, 'GRAD' 0, 'opsz' 24; }
    #model-selection-container {
      display: flex;
      align-items: center;
      gap: 12px;
      flex-wrap: wrap;
      padding: 8px 0;
    }
    #model-selection {
      min-width: 260px;
      padding: 12px 14px;
      padding-right: 40px;
      border-radius: 12px;
      border: 1px solid #d6d6d6;
      background: inherit;
      color: inherit;
      appearance: none;
      -webkit-appearance: none;
      -moz-appearance: none;
      background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="%23888" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg>');
      background-repeat: no-repeat;
      background-position: right 12px center;
      background-size: 16px;
      font-size: 14px;
      line-height: 1.4;
      cursor: pointer;
      transition: border-color .15s ease, box-shadow .15s ease, background-color .15s ease;
    }
    #model-selection:hover { border-color: #bdbdbd; }
    #model-selection:focus { outline: none; border-color: #9fa8da; box-shadow: 0 0 0 3px color-mix(in oklab, CanvasText 15%, transparent); }
    #model-selection:disabled { cursor: not-allowed; opacity: .7; }
    #model-selection optgroup { font-weight: 600; color: inherit; }
    #model-selection option { color: CanvasText; }
    #loading-row {
      display: flex;
      align-items: center;
      gap: 10px;
      min-height: 28px;
    }
    #loading-message {
      font-size: 14px;
      opacity: 0.8;
      display: none;
    }
    .elevated-card {
      padding: 16px 18px;
      border-radius: 12px;
      border: 1px solid color-mix(in oklab, CanvasText 10%, transparent);
      background: color-mix(in oklab, Canvas 96%, CanvasText 4%);
    }
    #chat-container {
      height: 420px;
      overflow: auto;
      display: flex;
      flex-direction: column;
      gap: 8px;
    }
    .message {
      padding: 12px 14px;
      border-radius: 12px;
      line-height: 1.5;
      word-wrap: break-word;
      white-space: pre-wrap;
      max-width: 80%;
    }
    .user-message {
      align-self: flex-end;
      background: rgba(25, 118, 210, 0.12);
    }
    .ai-message {
      align-self: flex-start;
      background: rgba(0, 0, 0, 0.06);
    }
    #input-container {
      display: flex;
      gap: 12px;
      align-items: flex-end;
    }
    #user-input {
      flex: 1;
      min-height: 44px;
      padding: 12px 14px;
      border-radius: 12px;
      border: 1px solid #d6d6d6;
      background: inherit;
      color: inherit;
      resize: vertical;
    }
    .spinner {
      width: 24px;
      height: 24px;
      border: 3px solid color-mix(in oklab, CanvasText 30%, transparent);
      border-top-color: color-mix(in oklab, CanvasText 80%, transparent);
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }
    @keyframes spin {
      to { transform: rotate(360deg); }
    }
    button {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      border-radius: 12px;
      border: 1px solid color-mix(in oklab, CanvasText 20%, transparent);
      background: color-mix(in oklab, Canvas 94%, CanvasText 6%);
      color: inherit;
      cursor: pointer;
      transition: background-color .2s ease, border-color .2s ease, box-shadow .2s ease, transform .05s ease;
    }
    button:hover { border-color: color-mix(in oklab, CanvasText 35%, transparent); }
    button:active { transform: translateY(1px); }
    button:disabled { opacity: 0.6; cursor: not-allowed; }

    #load-model-button {
      border: none;
      background: #1a73e8;
      color: #fff;
      box-shadow: 0 1px 2px rgba(0,0,0,.2), 0 2px 6px rgba(26,115,232,.3);
      font-weight: 600;
    }
    #load-model-button:hover {
      background: #1669c1;
      box-shadow: 0 2px 4px rgba(0,0,0,.25), 0 4px 10px rgba(26,115,232,.35);
    }
    #load-model-button:active { background: #155ab0; }
    #load-model-button:disabled {
      background: color-mix(in oklab, #1a73e8 40%, Canvas 60%);
      color: color-mix(in oklab, #fff 70%, CanvasText 30%);
      box-shadow: none;
    }
    @media (min-width: 768px) {
      main.container { padding: 28px 32px 36px; gap: 24px; }
      .elevated-card { padding: 20px 22px; }
      .message { padding: 14px 16px; }
      #user-input { min-height: 52px; padding: 14px 16px; }
      button { padding: 12px 18px; }
      #model-selection { padding: 12px 16px; }
    }
    /* Markdown styling */
    .message h1,.message h2,.message h3{margin:0.6em 0 0.4em;}
    .message h4,.message h5,.message h6{margin:0.5em 0 0.3em;}
    .message h1{font-size:1.3rem;}
    .message h2{font-size:1.2rem;}
    .message h3{font-size:1.1rem;}
    .message pre{
      background: color-mix(in oklab, CanvasText 10%, transparent);
      padding: 12px 14px;
      border-radius: 10px;
      overflow: auto;
      line-height: 1.4;
    }
    .message code{
      background: color-mix(in oklab, CanvasText 10%, transparent);
      padding: 2px 6px;
      border-radius: 8px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace;
    }
    .message pre code{ background: transparent; padding: 0; }
    .message a{ color: #1a73e8; text-decoration: underline; }
    .message ul,.message ol{ padding-left: 1.25em; margin: 0.2em 0 0.6em; }
    .message li{ margin: 0.15em 0; }
    .message blockquote{
      border-left: 3px solid color-mix(in oklab, CanvasText 25%, transparent);
      padding: 6px 10px;
      margin: 0.4em 0;
      color: color-mix(in oklab, CanvasText 80%, transparent);
    }
  </style>
</head>
<body>
  <main class="container">
    <header class="app-header">
      <span class="material-symbols-rounded">smart_toy</span>
      <h1>WebLLM Chat</h1>
    </header>

    <section id="model-selection-container">
      <label for="model-selection">选择模型:</label>
      <select id="model-selection">
        <optgroup label="Llama-3.2 (1B) - 轻量级">
          <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC">Llama-3.2-1B q4f16_1 (879MB VRAM)</option>
          <option value="Llama-3.2-1B-Instruct-q4f32_1-MLC">Llama-3.2-1B q4f32_1 (1.1GB VRAM)</option>
        </optgroup>
        <optgroup label="Llama-3.2 (3B) - 平衡型">
          <option value="Llama-3.2-3B-Instruct-q4f16_1-MLC">Llama-3.2-3B q4f16_1 (2.3GB VRAM)</option>
          <option value="Llama-3.2-3B-Instruct-q4f32_1-MLC">Llama-3.2-3B q4f32_1 (3GB VRAM)</option>
        </optgroup>
        <optgroup label="Llama-3.1 (8B) - 高性能">
          <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC">Llama-3.1-8B q4f16_1 (5GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC">Llama-3.1-8B q4f32_1 (6.1GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f16_1-MLC-1k">Llama-3.1-8B q4f16_1-1k (4.6GB VRAM)</option>
          <option value="Llama-3.1-8B-Instruct-q4f32_1-MLC-1k">Llama-3.1-8B q4f32_1-1k (5.3GB VRAM)</option>
        </optgroup>
        <optgroup label="DeepSeek-R1 - 推理专家">
          <option value="DeepSeek-R1-Distill-Qwen-7B-q4f16_1-MLC">DeepSeek-R1-Qwen-7B q4f16_1 (5.1GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Qwen-7B-q4f32_1-MLC">DeepSeek-R1-Qwen-7B q4f32_1 (5.9GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC">DeepSeek-R1-Llama-8B q4f16_1 (5GB VRAM)</option>
          <option value="DeepSeek-R1-Distill-Llama-8B-q4f32_1-MLC">DeepSeek-R1-Llama-8B q4f32_1 (6.1GB VRAM)</option>
        </optgroup>
        <optgroup label="Hermes-3 - 通用助手">
          <option value="Hermes-3-Llama-3.2-3B-q4f16_1-MLC">Hermes-3-Llama-3.2-3B q4f16_1 (2.3GB VRAM)</option>
          <option value="Hermes-3-Llama-3.2-3B-q4f32_1-MLC">Hermes-3-Llama-3.2-3B q4f32_1 (3GB VRAM)</option>
          <option value="Hermes-3-Llama-3.1-8B-q4f16_1-MLC">Hermes-3-Llama-3.1-8B q4f16_1 (4.9GB VRAM)</option>
          <option value="Hermes-3-Llama-3.1-8B-q4f32_1-MLC">Hermes-3-Llama-3.1-8B q4f32_1 (5.8GB VRAM)</option>
        </optgroup>
        <optgroup label="Phi-3.5 - 微软模型">
          <option value="Phi-3.5-mini-instruct-q4f16_1-MLC">Phi-3.5-mini q4f16_1 (3.7GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f32_1-MLC">Phi-3.5-mini q4f32_1 (5.5GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f16_1-MLC-1k">Phi-3.5-mini q4f16_1-1k (2.5GB VRAM)</option>
          <option value="Phi-3.5-mini-instruct-q4f32_1-MLC-1k">Phi-3.5-mini q4f32_1-1k (3.2GB VRAM)</option>
        </optgroup>
        <optgroup label="Mistral - 欧洲模型">
          <option value="Hermes-2-Pro-Mistral-7B-q4f16_1-MLC">Hermes-2-Pro-Mistral-7B q4f16_1 (4GB VRAM)</option>
        </optgroup>
        <optgroup label="视觉模型 (VLM)">
          <option value="Phi-3.5-vision-instruct-q4f16_1-MLC">Phi-3.5-vision q4f16_1 (4GB VRAM)</option>
          <option value="Phi-3.5-vision-instruct-q4f32_1-MLC">Phi-3.5-vision q4f32_1 (5.9GB VRAM)</option>
        </optgroup>
      </select>
      <button id="load-model-button" disabled>
        <span class="material-symbols-rounded">download</span>
        加载模型
      </button>
    </section>

    <div id="loading-row">
      <div id="model-progress" class="spinner" style="display: none;"></div>
      <div id="loading-message" class="loading"></div>
    </div>

    <div class="elevated-card">
      <div id="chat-container"></div>
    </div>

    <div id="input-container">
      <textarea id="user-input" placeholder="输入消息…（按 Enter 发送）"></textarea>
      <button id="send-button" disabled>
        <span class="material-symbols-rounded">send</span>
        发送
      </button>
    </div>
    <script type="module">
let webllm = null;

const chatContainer = document.getElementById('chat-container');
const userInput = document.getElementById('user-input');
const sendButton = document.getElementById('send-button');
const modelSelection = document.getElementById('model-selection');
const loadModelButton = document.getElementById('load-model-button');
const loadingMessage = document.getElementById('loading-message');
const modelProgress = document.getElementById('model-progress');

let messages = [
  { role: "system", content: "You are a helpful AI assistant." }
];

let engine = null;

function initProgressCallback(report) {
  const progress = report.progress ? ` (${(report.progress * 100).toFixed(1)}%)` : '';
  loadingMessage.textContent = `正在加载模型: ${report.text}${progress}`;
  loadingMessage.style.display = 'block';
  modelProgress.style.display = 'inline-block';
  chatContainer.scrollTop = chatContainer.scrollHeight;
}

async function loadModel() {
  // Lazy import WebLLM when needed
  if (!webllm) {
    try {
      webllm = await import("https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.79/+esm");
    } catch (e) {
      loadingMessage.textContent = "无法加载 WebLLM（可能被预览环境或网络限制拦截）。请在外部浏览器打开 http://127.0.0.1:8001 再试。";
      loadingMessage.style.display = "block";
      return;
    }
  }
  // Clear chat container
  chatContainer.innerHTML = '';
  
  // Reset messages
  messages = [
    { role: "system", content: "You are a helpful AI assistant." }
  ];
  
  // Get selected model
  const selectedModel = modelSelection.value;
  
  // Disable UI elements during loading
  modelSelection.disabled = true;
  loadModelButton.disabled = true;
  sendButton.disabled = true;
  modelProgress.style.display = 'inline-block';
  loadingMessage.style.display = 'block';
  loadingMessage.textContent = '正在准备加载模型…';
  
  try {
    // Create or reload engine
    if (engine === null) {
      engine = await webllm.CreateMLCEngine(
        selectedModel,
        { initProgressCallback: initProgressCallback }
      );
    } else {
      await engine.reload(selectedModel, { initProgressCallback: initProgressCallback });
    }
    
    // Hide loading indicators
    loadingMessage.style.display = 'none';
    modelProgress.style.display = 'none';
    
    // Enable send button
    sendButton.disabled = false;
    
    // Add welcome message with model info
    addMessage('ai', `✅ 模型 ${selectedModel} 已加载成功！\n\n💡 提示：\n- 当前模型支持中文和英文对话\n- 点击发送按钮或按 Enter 键发送消息\n- 如需切换模型，请重新选择并点击"加载模型"\n\n有什么我可以帮你的吗？`);
  } catch (error) {
    console.error('Failed to load model:', error);
    
    let errorMessage = '模型加载失败';
    if (error.message && error.message.includes('WebGPU')) {
      errorMessage = '❌ 您的浏览器不支持 WebGPU，请使用 Chrome/Edge 最新版本';
    } else if (error.message && error.message.includes('Network')) {
      errorMessage = '❌ 网络连接失败，请检查网络后重试';
    } else if (error.message && error.message.includes('memory')) {
      errorMessage = '❌ 内存不足，请尝试选择更小的模型';
    } else if (error.message) {
      errorMessage = `❌ 加载失败：${error.message}`;
    }
    
    loadingMessage.textContent = errorMessage;
    loadingMessage.style.display = 'block';
    modelProgress.style.display = 'none';
    
    // Enable model selection
    modelSelection.disabled = false;
    loadModelButton.disabled = false;
  }
}

// Enable the load model button when the page loads
window.addEventListener('load', async () => {
  loadModelButton.disabled = false;
  
  // Check WebGPU support
  if (!navigator.gpu) {
    addMessage('ai', '⚠️ 警告：您的浏览器不支持 WebGPU\n\nWebLLM 需要 WebGPU 支持才能运行。请使用以下浏览器：\n- Chrome 113+ 或 Edge 113+\n- Chrome Canary (最新测试版)\n- 其他支持 WebGPU 的现代浏览器\n\n当前浏览器将无法加载模型。');
    loadModelButton.disabled = true;
  } else {
    addMessage('ai', '👋 欢迎使用 WebLLM AI 聊天！\n\n📋 使用步骤：\n1. 从下拉菜单选择一个模型\n2. 点击"加载模型"按钮\n3. 等待模型下载和初始化\n4. 开始对话！\n\n💡 建议：\n- 新手推荐 Llama-3.2-1B 系列（内存占用小）\n- 需要更好性能选择 Llama-3.1-8B 系列\n- 首次加载需要下载模型，请耐心等待');
  }
});

/* Markdown renderer (escapes HTML, supports code fences, inline code, headings, lists, links, hr) */
function renderMarkdown(md) {
  if (!md) return '';
  let s = String(md)
    .replace(/&/g, '&')
    .replace(/</g, '<')
    .replace(/>/g, '>');

  // Extract fenced code blocks first to avoid further processing inside them
  const blocks = [];
  s = s.replace(/```(\w+)?\n([\s\S]*?)```/g, (m, lang, code) => {
    const idx = blocks.length;
    const ln = (lang || '').toLowerCase();
    blocks.push(`<pre><code class="language-${ln}">${code}</code></pre>`);
    return `\u0000CODEBLOCK_${idx}\u0000`;
  });

  // Inline code
  s = s.replace(/`([^`]+)`/g, '<code>$1</code>');

  // Headings
  s = s.replace(/^(#{1,6})\s*(.+)$/gm, (m, hashes, title) => {
    const level = hashes.length;
    return `<h${level}>${title}</h${level}>`;
  });

  // Blockquotes
  s = s.replace(/^>\s?(.*)$/gm, '<blockquote>$1</blockquote>');

  // Horizontal rule
  s = s.replace(/^\s*(?:-{3,}|\*{3,})\s*$/gm, '<hr/>');

  // Links [text](url) with basic allowlist and escaping
  s = s.replace(/\[([^\]]+)\]\(([^)]+)\)/g, (m, text, url) => {
    try {
      const u = new URL(url, window.location.href);
      const ok = u.protocol === 'http:' || u.protocol === 'https:' || u.protocol === 'mailto:';
      if (!ok) return text;
      const escText = String(text).replace(/"/g, '"');
      const href = u.href.replace(/&/g, '&').replace(/"/g, '"');
      return `<a href="${href}" target="_blank" rel="noopener noreferrer">${escText}</a>`;
    } catch {
      return text;
    }
  });

  // Simple list handling and line breaks, preserving code blocks
  const lines = s.split('\n');
  let out = '';
  let inUl = false, inOl = false;
  const tokenRe = /^\u0000CODEBLOCK_(\d+)\u0000$/;

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];

    if (tokenRe.test(line)) {
      if (inUl) { out += '</ul>'; inUl = false; }
      if (inOl) { out += '</ol>'; inOl = false; }
      out += line;
      continue;
    }

    if (/^\s*[-*+]\s+/.test(line)) {
      if (!inUl) { if (inOl) { out += '</ol>'; inOl = false; } out += '<ul>'; inUl = true; }
      out += `<li>${line.replace(/^\s*[-*+]\s+/, '')}</li>`;
      continue;
    }
    if (/^\s*\d+\.\s+/.test(line)) {
      if (!inOl) { if (inUl) { out += '</ul>'; inUl = false; } out += '<ol>'; inOl = true; }
      out += `<li>${line.replace(/^\s*\d+\.\s+/, '')}</li>`;
      continue;
    }

    if (inUl) { out += '</ul>'; inUl = false; }
    if (inOl) { out += '</ol>'; inOl = false; }

    if (line.trim() === '') {
      out += '<br/>';
    } else {
      out += line + '<br/>';
    }
  }
  if (inUl) out += '</ul>';
  if (inOl) out += '</ol>';

  // Restore code blocks
  out = out.replace(/\u0000CODEBLOCK_(\d+)\u0000/g, (_, i) => blocks[Number(i)] || '');

  return out;
}

// Add message to chat container
function addMessage(role, content) {
  const messageElement = document.createElement('div');
  messageElement.className = `message ${role}-message`;
  if (role === 'ai') {
    messageElement.innerHTML = renderMarkdown(content || '');
  } else {
    messageElement.textContent = content || '';
  }
  chatContainer.appendChild(messageElement);
  chatContainer.scrollTop = chatContainer.scrollHeight;
}

// Send message to AI
async function sendChatMessage() {
  const message = (userInput.value || '').trim();
  if (!message || !engine) return;

  // Add user message to UI
  addMessage('user', message);
  
  // Clear input
  userInput.value = '';
  
  // Add user message to history
  messages.push({ role: 'user', content: message });
  
  // Show loading indicator
  const loadingEl = document.createElement('div');
  loadingEl.className = 'message loading';
  loadingEl.textContent = 'AI 正在思考...';
  chatContainer.appendChild(loadingEl);
  chatContainer.scrollTop = chatContainer.scrollHeight;
  
  try {
    // Generate AI response
    const reply = await engine.chat.completions.create({
      messages: messages,
    });
    
    // Remove loading message
    chatContainer.removeChild(loadingEl);
    
    // Add AI response to UI
    const aiMessage = reply.choices[0].message.content;
    addMessage('ai', aiMessage);
    
    // Add AI response to history
    messages.push({ role: 'assistant', content: aiMessage });
  } catch (error) {
    console.error('Failed to generate response:', error);
    chatContainer.removeChild(loadingEl);
    addMessage('ai', '抱歉，我无法生成回复。请稍后重试。');
  }
}

// Event listeners
sendButton.addEventListener('click', sendChatMessage);
userInput.addEventListener('keydown', (e) => {
  if (e.key === 'Enter' && !e.shiftKey) {
    e.preventDefault();
    sendChatMessage();
  }
});
loadModelButton.addEventListener('click', loadModel);

// Initialize the engine when the page loads
// window.addEventListener('load', initializeEngine);
    </script>
  </main>
</body>
</html>